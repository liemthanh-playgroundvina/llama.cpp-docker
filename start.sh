llama.cpp/server --model models/gemma-2b-it-q4_k_m.gguf --ctx-size 256 --n-gpu-layers -1 --host 0.0.0.0 --port 8900